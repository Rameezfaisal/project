# #### Step-3 -- Reading Source tables (Fixed Type Mismatch)

from pyspark.sql import functions as F
from datetime import date

# 1. Define Date/Year Cutoffs
today = F.current_date()
five_years_ago = F.date_sub(today, 365*6)

# CRITICAL FIX: Convert start_year to STRING.
# SAP stores MJAHR as string. Comparing Int (2020) >= String ('2020') kills performance.
# By making this a string, we enable Partition Pruning.
start_year_str = str(date.today().year - 6)

print(f"Filtering MSEG for MJAHR >= '{start_year_str}' (String Match)")

# --- 1. Read MKPF (Header) ---
try:
    mkpf_ecc = spark.read.table(paths["ecc"]["mkpf"]) \
                    .select("mblnr", "mjahr", "bldat") \
                    .filter(F.col("bldat") >= five_years_ago)
except:
    mkpf_ecc = None

try:
    mkpf_s4h = spark.read.table(paths["s4h"]["mkpf"]) \
                    .select("mblnr", "mjahr", "bldat") \
                    .filter(F.col("bldat") >= five_years_ago)
except:
    mkpf_s4h = None

# Safe Union MKPF
if mkpf_ecc and mkpf_s4h:
    mkpf = mkpf_ecc.unionByName(mkpf_s4h)
elif mkpf_ecc:
    mkpf = mkpf_ecc
else:
    mkpf = mkpf_s4h

# --- 2. Read MSEG (Item) ---
# FIX: Filter applied using STRING variable (start_year_str)
try:
    mseg_ecc = spark.read.table(paths["ecc"]["mseg"]) \
                    .select("mblnr", "mjahr", "zeile", "matnr", "menge", "shkzg", "bwart") \
                    .filter(F.col("mjahr") >= start_year_str) 
except:
    mseg_ecc = None

try:
    mseg_s4h = spark.read.table(paths["s4h"]["mseg"]) \
                    .select("mblnr", "mjahr", "zeile", "matnr", "menge", "shkzg", "bwart") \
                    .filter(F.col("mjahr") >= start_year_str)
except:
    mseg_s4h = None

# Safe Union MSEG
if mseg_ecc and mseg_s4h:
    mseg = mseg_ecc.unionByName(mseg_s4h)
elif mseg_ecc:
    mseg = mseg_ecc
else:
    mseg = mseg_s4h

# --- 3. Read Other Tables ---
def simple_union(key):
    try: df1 = spark.read.table(paths["ecc"][key])
    except: df1 = None
    try: df2 = spark.read.table(paths["s4h"][key])
    except: df2 = None
    if df1 and df2: return df1.unionByName(df2, allowMissingColumns=True)
    return df1 if df1 else df2

makt      = simple_union("makt")
forecast  = simple_union("zmmforecast")
mard      = simple_union("mard")
po        = simple_union("zpo_hstry")
ekpo      = simple_union("ekpo")

spares = spark.table("wsf_silk_glb_dt_qa.lhg_glb.eng.rpt_csbg_spares_deliveries")
suppliers_by_plant = spark.table("lhs_glb.omat_test.rpt_omat_buynha_suppliers_plants")

# Re-establish Scope
nha = spark.table("lhs_glb.omat_test.rpt_omat_obpn_buy_nha_dtls") \
           .select("cmpprtno", "nha", "isprocessed") \
           .filter(F.col("isprocessed") == "N") \
           .distinct()

