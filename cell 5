df_sprs_dmd = (
    df_nhalist.alias("a")
    .join(df_so.alias("b"),
          F.col("a.nha_cd") == F.col("b.matnr_cd"),
          "inner")
    .filter(
        (F.col("b.ordergroup") == "SO") &
        (F.col("b.orderstatus_cd") != "closed") &
        (F.col("b.deliveryduedate") >= dt1wk) &
        (F.col("b.deliveryduedate") <= dt26wk) &
        F.col("b.ordertype").isin("TA","ZCON","ZO09","ZO04","ZSD","ZSO","ZFO","ZUP")
    )
    .groupBy(F.col("b.matnr_cd"))
    .agg(
        F.sum(F.col("b.openqty")).alias("sprs_dmd_cd")
    )
)

df_nhalist_sprsdmd = (
    df_nhalist_mfgdmd.alias("a")
    .join(df_sprs_dmd.alias("b"),
          F.col("b.matnr_cd") == F.col("a.nha_cd"),
          "left")
    .select(
        "a.cmpprtno_cd",
        "a.nha_cd",
        "a.mfg_5yrs_consumption_cd",
        "a.mfg_dmd_cd",
        F.col("b.sprs_dmd_cd")
    )
)

df_nhalist_sprsdmd.createOrReplaceTempView("nhalist_sprsdmd_vw")








df_sprs_cons = (
    df_spares.alias("a")
    .join(df_nhalist.alias("b"),
          F.col("b.nha_cd") == F.col("a.material_cd"),
          "inner")
    .filter(
        F.col("a.source_doc") != "STO" &
        F.col("a.order_type").isin("TA","ZCON","ZO09","ZO04","ZSD","ZSO","ZFO","ZUP") &
        (F.col("a.actual_gi") > F.date_sub(F.current_date(), 365*5))
    )
    .groupBy(F.col("a.material_cd"))
    .agg(
        F.sum(F.col("a.qty_shipped")).alias("sprs_5yrs_consumption_cd")
    )
    .filter(F.col("sprs_5yrs_consumption_cd") > 0)
)

df_nhalist_sprscons = (
    df_nhalist_sprsdmd.alias("a")
    .join(df_sprs_cons.alias("b"),
          F.col("b.material_cd") == F.col("a.nha_cd"),
          "left")
    .select(
        "a.cmpprtno_cd",
        "a.nha_cd",
        "a.mfg_5yrs_consumption_cd",
        "a.mfg_dmd_cd",
        "a.sprs_dmd_cd",
        F.col("b.sprs_5yrs_consumption_cd")
    )
)

df_nhalist_sprscons.createOrReplaceTempView("nhalist_sprscons_vw")









df_mfg_12m = (
    df_mseg.alias("b")
    .join(df_makt.alias("c"), F.col("c.matnr_cd") == F.col("b.matnr_cd"), "inner")
    .join(df_mkpf.alias("d"), F.col("b.mblnr") == F.col("d.mblnr"), "inner")
    .join(df_nhalist.alias("a"), F.col("b.matnr_cd") == F.col("a.nha_cd"), "inner")
    .filter(
        F.col("b.bwart").isin("261","262") &
        (F.col("d.bldat") > F.date_sub(F.current_date(), 365)) &
        (F.col("d.bldat") < F.current_date()) &
        (F.col("b.cpudt_mkpf") > F.date_sub(F.current_date(), 365))
    )
    .groupBy(F.col("b.matnr_cd"))
    .agg(
        F.sum(
            F.when(F.col("b.shkzg_cd") == "s", -1 * F.col("b.menge"))
             .otherwise(F.col("b.menge"))
        ).alias("mfg_12mnths_consumption_cd")
    )
)

df_final_base = (
    df_nhalist_sprscons.alias("a")
    .join(df_mfg_12m.alias("b"),
          F.col("b.matnr_cd") == F.col("a.nha_cd"),
          "left")
    .select(
        "a.cmpprtno_cd",
        "a.nha_cd",
        "a.mfg_5yrs_consumption_cd",
        "a.mfg_dmd_cd",
        "a.sprs_dmd_cd",
        "a.sprs_5yrs_consumption_cd",
        F.col("b.mfg_12mnths_consumption_cd")
    )
    .filter(
        (F.abs(F.coalesce("mfg_5yrs_consumption_cd", F.lit(0))) +
         F.abs(F.coalesce("mfg_dmd_cd", F.lit(0))) +
         F.abs(F.coalesce("sprs_dmd_cd", F.lit(0))) +
         F.abs(F.coalesce("sprs_5yrs_consumption_cd", F.lit(0)))) > 0
    )
)

df_final_base.createOrReplaceTempView("nhalist_final_vw")








spark.sql(f"TRUNCATE TABLE {tgt_dmd_cons}")

spark.sql(f"""
INSERT INTO {tgt_dmd_cons} (
    cmpprtno,
    nha,
    mfg_5yrs_consumption,
    mfg_dmd,
    sprs_dmd,
    sprs_5yrs_consumption,
    mfg_12mnths_consumption,
    last_modified_on
)
SELECT
    cmpprtno_cd,
    nha_cd,
    mfg_5yrs_consumption_cd,
    mfg_dmd_cd,
    sprs_dmd_cd,
    sprs_5yrs_consumption_cd,
    mfg_12mnths_consumption_cd,
    current_date()
FROM nhalist_final_vw
""")









obprt_cnt = df_nhamainlist.select("cmpprtno_cd").distinct().count()
nha_cnt = df_nhamainlist.select("nha_cd").distinct().count()

df_log_start = spark.createDataFrame(
    [("sp_omat_updatedmd_cons_mknha_wklyrefresh - started", 1, obprt_cnt, nha_cnt, F.current_timestamp())],
    ["message","status","obprt_cnt","nha_cnt","logged_at"]
)

df_log_end = spark.createDataFrame(
    [("sp_omat_updatedmd_cons_mknha_wklyrefresh - end", 1, obprt_cnt, nha_cnt, F.current_timestamp())],
    ["message","status","obprt_cnt","nha_cnt","logged_at"]
)

df_log_start.write.format("delta").mode("append").saveAsTable(tgt_log)
df_log_end.write.format("delta").mode("append").saveAsTable(tgt_log)