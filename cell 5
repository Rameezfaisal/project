# #### Step-3 -- Reading Source tables (Simplified & Optimized)

from pyspark.sql import functions as F
from datetime import date

# Define the date cutoffs
today = F.current_date()
five_years_ago = F.date_sub(today, 365*6)
start_year = date.today().year - 6

# --- 1. Read MKPF (Header) ---
# We read ECC and S4H separately, select only required cols, filter, then union.
try:
    mkpf_ecc = spark.read.table(paths["ecc"]["mkpf"]) \
                    .select("mblnr", "mjahr", "bldat") \
                    .filter(F.col("bldat") >= five_years_ago)
except:
    mkpf_ecc = None

try:
    mkpf_s4h = spark.read.table(paths["s4h"]["mkpf"]) \
                    .select("mblnr", "mjahr", "bldat") \
                    .filter(F.col("bldat") >= five_years_ago)
except:
    mkpf_s4h = None

# Safe Union
if mkpf_ecc and mkpf_s4h:
    mkpf = mkpf_ecc.unionByName(mkpf_s4h)
elif mkpf_ecc:
    mkpf = mkpf_ecc
else:
    mkpf = mkpf_s4h

# --- 2. Read MSEG (Item) ---
# We read ECC and S4H separately, select only required cols, filter by Year, then union.
try:
    mseg_ecc = spark.read.table(paths["ecc"]["mseg"]) \
                    .select("mblnr", "mjahr", "zeile", "matnr", "menge", "shkzg", "bwart") \
                    .filter(F.col("mjahr") >= start_year)
except:
    mseg_ecc = None

try:
    mseg_s4h = spark.read.table(paths["s4h"]["mseg"]) \
                    .select("mblnr", "mjahr", "zeile", "matnr", "menge", "shkzg", "bwart") \
                    .filter(F.col("mjahr") >= start_year)
except:
    mseg_s4h = None

# Safe Union
if mseg_ecc and mseg_s4h:
    mseg = mseg_ecc.unionByName(mseg_s4h)
elif mseg_ecc:
    mseg = mseg_ecc
else:
    mseg = mseg_s4h

# --- 3. Read Other Tables (Standard Union) ---
# Keeping the original simple helper for the smaller tables
def simple_union(key):
    try: df1 = spark.read.table(paths["ecc"][key])
    except: df1 = None
    try: df2 = spark.read.table(paths["s4h"][key])
    except: df2 = None
    if df1 and df2: return df1.unionByName(df2, allowMissingColumns=True)
    return df1 if df1 else df2

makt      = simple_union("makt")
forecast  = simple_union("zmmforecast")
mard      = simple_union("mard")
po        = simple_union("zpo_hstry")
ekpo      = simple_union("ekpo")

spares = spark.table("wsf_silk_glb_dt_qa.lhg_glb.eng.rpt_csbg_spares_deliveries")
suppliers_by_plant = spark.table("lhs_glb.omat_test.rpt_omat_buynha_suppliers_plants")

# Re-establish Scope
nha = spark.table("lhs_glb.omat_test.rpt_omat_obpn_buy_nha_dtls") \
           .select("cmpprtno", "nha", "isprocessed") \
           .filter(F.col("isprocessed") == "N") \
           .distinct()




