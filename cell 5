from datetime import datetime

# ---- Calculate counts (SAP OBPRTCnt / NHACnt equivalents) ----
obprtno_cnt = df_nhamainlist.select("cmpprtno_cd").distinct().count()
obpr_cnt = df_nhamainlist.select("nha_cd").distinct().count()
obprtno_nha_cnt = df_nhamainlist.count()   # total CMPPRTNOâ€“NHA pairs

# materialize timestamp once
ts_now = datetime.now()

# ---- Create start log row (use Python values, not Spark Columns) ----
df_log_start = spark.createDataFrame(
    [(
        ts_now,
        obprtno_cnt,
        obprtno_nha_cnt,
        obpr_cnt,
        "SP_OMAT_UPDATEDMD_CONS_MKNHA_WklyRefresh - Started"
    )],
    ["executed_on", "obprtno_cnt", "obprtno_nha_cnt", "obpr_cnt", "program_name"]
)

# ---- Create end log row (new timestamp) ----
df_log_end = spark.createDataFrame(
    [(
        datetime.now(),
        obprtno_cnt,
        obprtno_nha_cnt,
        obpr_cnt,
        "SP_OMAT_UPDATEDMD_CONS_MKNHA_WklyRefresh - End"
    )],
    ["executed_on", "obprtno_cnt", "obprtno_nha_cnt", "obpr_cnt", "program_name"]
)

# ---- Append both records to Delta log table ----
df_log_start.write.format("delta").mode("append").saveAsTable("eng_test.rpt_omat_log_dtls")
df_log_end.write.format("delta").mode("append").saveAsTable("eng_test.rpt_omat_log_dtls")