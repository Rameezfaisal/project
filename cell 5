# #### Step-3 -- Reading Source tables (Robust Optimized Strategy)

from datetime import date

# 1. Calculate Pruning Year
current_year = date.today().year
start_year = current_year - 6 
print(f"Pruning strategy set for years >= {start_year}")

# 2. Robust Read Function
def read_filtered_union(table_key, year_col=None, min_year=None, date_col=None, min_date=None):
    ecc_path = paths["ecc"].get(table_key)
    s4h_path = paths["s4h"].get(table_key)
    
    dfs = []
    for path in [ecc_path, s4h_path]:
        # Block A: Attempt to Read Table
        try:
            df = spark.read.table(path)
        except Exception as e:
            print(f"Skipping {path}: Table not found or inaccessible.")
            continue

        # Block B: Attempt to Apply Optimization Filters
        # We wrap this separately so if filtering fails, we don't lose the data.
        try:
            if year_col and min_year:
                # Check if column exists to avoid crashing
                if year_col in df.columns:
                    df = df.filter(F.col(year_col) >= min_year)
                elif year_col.upper() in df.columns:
                    df = df.filter(F.col(year_col.upper()) >= min_year)
                else:
                    print(f"Warning: Column '{year_col}' not found in {path}. Skipping partition pruning.")

            if date_col and min_date:
                # Check if column exists
                if date_col in df.columns:
                    df = df.filter(F.col(date_col) >= min_date)
                elif date_col.upper() in df.columns:
                    df = df.filter(F.col(date_col.upper()) >= min_date)
                else:
                    print(f"Warning: Column '{date_col}' not found in {path}. Skipping date filtering.")
                    
        except Exception as e:
            print(f"Warning: Optimization filter failed for {path}. Proceeding with full data. Error: {e}")
        
        # Add the dataframe (filtered or not) to our list
        dfs.append(df)
            
    if not dfs:
        # If we get here, it means the table read (Block A) failed for BOTH paths.
        raise ValueError(f"No data found for {table_key} in either ECC or S4H paths.")
        
    # Align schemas and union
    all_cols = sorted(set(dfs[0].columns) | set(dfs[1].columns)) if len(dfs) > 1 else dfs[0].columns
    
    aligned_dfs = [
        df.select([F.col(c) if c in df.columns else F.lit(None).alias(c) for c in all_cols])
        for df in dfs
    ]
    
    return aligned_dfs[0].unionByName(aligned_dfs[1]) if len(aligned_dfs) > 1 else aligned_dfs[0]

# 3. Read Tables
# We remove 'mjahr' from MKPF call just in case it's missing there, relying on 'bldat' instead.
mkpf = read_filtered_union("mkpf", date_col="bldat", min_date=F.date_sub(F.current_date(), 365*6))

# For MSEG, we keep the year filter but the new function will handle it gracefully if missing
mseg_full = read_filtered_union("mseg", year_col="mjahr", min_year=start_year)
mseg = mseg_full.select("mblnr", "mjahr", "zeile", "matnr", "menge", "shkzg", "bwart")

# Read other tables normally
makt      = read_union_fast("makt")
forecast  = read_union_fast("zmmforecast")
mard      = read_union_fast("mard")
po        = read_union_fast("zpo_hstry")
ekpo      = read_union_fast("ekpo")

spares = spark.table("wsf_silk_glb_dt_qa.lhg_glb.eng.rpt_csbg_spares_deliveries")
suppliers_by_plant = spark.table("lhs_glb.omat_test.rpt_omat_buynha_suppliers_plants")

# Re-establish Scope
nha = spark.table("lhs_glb.omat_test.rpt_omat_obpn_buy_nha_dtls") \
           .select("cmpprtno", "nha", "isprocessed") \
           .filter(F.col("isprocessed") == "N") \
           .distinct()
