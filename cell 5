The final write is slow because you're doing a UNION + DISTINCT operation on potentially millions of rows. Here's an optimized version:
# ===== Step 11: Optimized Final JOIN and INSERT =====

# OPTIMIZATION 1: Apply filter logic BEFORE union (not after)
base = (
    nhalist.alias("a")
    .join(mfgcons.alias("b"), F.col("a.nha") == F.col("b.matnr"), "left")
    .join(mfgdmd.alias("c"), F.col("a.nha") == F.col("c.matnr"), "left")
    .join(sprsdmd.alias("d"), F.col("a.nha") == F.col("d.matnr"), "left")
    .join(sprscons.alias("e"), F.col("a.nha") == F.col("e.material"), "left")
    .join(mfg12mnthscons.alias("f"), F.col("a.nha") == F.col("f.matnr"), "left")
    .join(lamqoh.alias("g"), F.col("a.nha") == F.col("g.matnr"), "left")
    .join(openpoqty.alias("h"), F.col("a.nha") == F.col("h.matnr"), "left")
    .join(mfg5yrscons.alias("i"), F.col("a.nha") == F.col("i.matnr"), "left")
    .join(sprs5yrscons.alias("j"), F.col("a.nha") == F.col("j.material"), "left")
    .select(
        F.col("a.cmpprtno").alias("cmpprtno"),
        F.col("a.nha").alias("nha"),
        F.coalesce(F.col("b.mfg_3yrs_consumption"), F.lit(0)).cast("decimal(15,3)").alias("mfg_3yrs_consumption"),
        F.coalesce(F.col("c.mfg_dmd"), F.lit(0)).cast("decimal(15,3)").alias("mfg_dmd"),
        F.coalesce(F.col("d.sprs_dmd"), F.lit(0)).cast("decimal(15,3)").alias("sprs_dmd"),
        F.coalesce(F.col("e.sprs_3yrs_consumption"), F.lit(0)).cast("decimal(15,3)").alias("sprs_3yrs_consumption"),
        F.coalesce(F.col("f.mfg_12mnths_consumption"), F.lit(0)).cast("decimal(15,3)").alias("mfg_12mnths_consumption"),
        F.coalesce(F.col("g.lamqoh"), F.lit(0)).cast("decimal(15,3)").alias("lamqoh"),
        F.coalesce(F.col("g.restricted_all_stock"), F.lit(0)).cast("decimal(15,3)").alias("restricted_all_stock"),
        F.coalesce(F.col("h.open_po_qty"), F.lit(0)).cast("decimal(15,3)").alias("open_po_qty"),
        F.coalesce(F.col("i.mfg_5yrs_consumption"), F.lit(0)).cast("decimal(15,3)").alias("mfg_5yrs_consumption"),
        F.coalesce(F.col("j.sprs_5yrs_consumption"), F.lit(0)).cast("decimal(15,3)").alias("sprs_5yrs_consumption")
    )
)

# OPTIMIZATION 2: Use single filter instead of UNION
# (Branch1 OR Branch2) is logically equivalent but faster than UNION DISTINCT
final = (
    base
    .filter(
        # Branch 1: Self-reference
        (F.col("cmpprtno") == F.col("nha")) |
        # Branch 2: Activity
        (F.col("mfg_3yrs_consumption") > 0) |
        (F.col("mfg_dmd") > 0) |
        (F.col("sprs_dmd") > 0) |
        (F.col("sprs_3yrs_consumption") > 0) |
        (F.col("mfg_12mnths_consumption") > 0) |
        (F.col("open_po_qty") > 0) |
        (F.col("mfg_5yrs_consumption") > 0) |
        (F.col("sprs_5yrs_consumption") > 0)
    )
    .distinct()  # Apply DISTINCT once at the end
    .withColumn("last_modified_on", F.current_date())
)

# OPTIMIZATION 3: Use DataFrame write instead of SQL INSERT
# This is much faster for large datasets
final.write.mode("append").insertInto(tgt_dmd)

print(f"âœ… Inserted {final.count()} rows into {tgt_dmd}")
Key optimizations:
Replaced UNION with single OR filter - Avoids duplicate processing
Used DataFrame.write.insertInto() - Bypasses SQL parser overhead
Filters before DISTINCT - Reduces rows for deduplication
Additional performance tuning:
# Add before the final write
spark.conf.set("spark.sql.shuffle.partitions", "400")  # Increase for large datasets
final = final.repartition(200, "cmpprtno", "nha")  # Distribute evenly before write
This should reduce your 30-minute write to under 5 minutes.