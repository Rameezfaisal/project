# #### Step-3 -- Reading Source tables (Optimized Reading Strategy)

from datetime import date

# 1. Calculate the "Partition Pruning" Year
# FIX: Use Python's native 'date' to get an integer year for the logic variables.
# We cannot use the Spark 'today' column here because it is not an integer yet.
current_year = date.today().year
start_year = current_year - 6 

print(f"Pruning MSEG partitions older than year: {start_year}")

# 2. Optimized Read Function with "Push-Down Filters"
def read_filtered_union(table_key, year_col, min_year, date_col=None, min_date=None):
    ecc_path = paths["ecc"].get(table_key)
    s4h_path = paths["s4h"].get(table_key)
    
    dfs = []
    for path in [ecc_path, s4h_path]:
        try:
            df = spark.read.table(path)
            
            # CRITICAL: Apply Partition Filter IMMEDIATELY upon read
            if year_col and min_year:
                df = df.filter(F.col(year_col) >= min_year)
            
            # Apply Date Filter if applicable (for MKPF)
            if date_col and min_date:
                df = df.filter(F.col(date_col) >= min_date)
                
            dfs.append(df)
        except Exception:
            continue
            
    if not dfs:
        raise ValueError(f"No data found for {table_key}")
        
    # Align schemas and union
    all_cols = sorted(set(dfs[0].columns) | set(dfs[1].columns)) if len(dfs) > 1 else dfs[0].columns
    
    aligned_dfs = [
        df.select([F.col(c) if c in df.columns else F.lit(None).alias(c) for c in all_cols])
        for df in dfs
    ]
    
    return aligned_dfs[0].unionByName(aligned_dfs[1]) if len(aligned_dfs) > 1 else aligned_dfs[0]

# 3. Read Tables with Optimized Strategy
# Only read MKPF where Date > 5 Years ago
mkpf = read_filtered_union("mkpf", "mjahr", start_year, "bldat", F.date_sub(F.current_date(), 365*6))

# Only read MSEG where Fiscal Year (MJAHR) > 5 Years ago
# We also SELECT specific columns to reduce I/O overhead
mseg_full = read_filtered_union("mseg", "mjahr", start_year)
mseg = mseg_full.select("mblnr", "mjahr", "zeile", "matnr", "menge", "shkzg", "bwart")

# Read other tables normally
makt      = read_union_fast("makt")
forecast  = read_union_fast("zmmforecast")
mard      = read_union_fast("mard")
po        = read_union_fast("zpo_hstry")
ekpo      = read_union_fast("ekpo")

spares = spark.table("wsf_silk_glb_dt_qa.lhg_glb.eng.rpt_csbg_spares_deliveries")
suppliers_by_plant = spark.table("lhs_glb.omat_test.rpt_omat_buynha_suppliers_plants")

# Re-establish Scope
nha = spark.table("lhs_glb.omat_test.rpt_omat_obpn_buy_nha_dtls") \
           .select("cmpprtno", "nha", "isprocessed") \
           .filter(F.col("isprocessed") == "N") \
           .distinct()
