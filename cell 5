# #### Step-4 -- Creating NHALIST Temp table (Optimized)
# Optimization: Cache this table. It is used 9 times. Caching prevents 9 re-scans.

nhalist = nha.select("cmpprtno", "nha").distinct().cache()

# Force materialization to ensure it's in memory before heavy processing starts
print(f"Items to process: {nhalist.count()}")









# #### Step-5 -- Calculating Manufacturing 3-year Consumption (Optimized)

mfg3 = (
    mseg.join(mkpf, "mblnr")
        # OPTIMIZATION: Broadcast the small nhalist
        .join(F.broadcast(nhalist), mseg.matnr == nhalist.nha) 
        .filter(
            (mseg.bwart.isin("261","262")) &
            (mkpf.bldat > F.date_sub(today, 365*3))
        )
        .groupBy(nhalist.cmpprtno.alias("cmpprtno_cd"), mseg.matnr.alias("nha_cd"))
        .agg(F.sum(F.when(mseg.shkzg=="S",-mseg.menge).otherwise(mseg.menge)).alias("mfg_3yrs_consumption"))
)









# Clean up memory
spark.sql(f""" INSERT INTO ... """) # (Your insert SQL)
nhalist.unpersist()
