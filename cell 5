I have a PySpark notebook migrated from an SAP HANA stored procedure. The notebook performs multiple transformations and writes one or more output tables.
I want to generate comprehensive data engineering unit tests using SQL queries for this notebook.
Act as a Senior Data Engineer and analyze both the PySpark code and the SAP logic to derive meaningful test cases.

1. Original SAP Procedure / Business Logic is in the file i will provide

üéØ Task:
Generate SQL-based data validation test queries to validate the notebook logic.
A. Input Data Validation
Queries to check input tables are not empty
Queries to detect NULLs in critical columns
B. Schema Validation
Queries to validate required columns exist in output tables
C. Transformation Logic Validation
Derive SQL tests by comparing SAP logic with PySpark transformations
Validate joins, filters, aggregations, calculations, and derived columns
D. Data Quality Checks
Queries to detect duplicate records based on business keys
Queries to detect invalid values (negative numbers where not allowed, unexpected categories, etc.)
E. Load Behavior Validation
Merge or Upsert ‚Üí query to ensure which data is updated
FULL LOAD ‚Üí query to ensure old data was replaced
APPEND LOAD ‚Üí query to ensure row count increased appropriately
F. Business Rule Validation
Write SQL queries validating business rules inferred from SAP logic
G. Edge Case Testing if required:
Queries to detect missing keys,
Queries to detect null timestamps,
Queries to detect unexpected dimension/category values,
Any other Queries if required.

‚ö†Ô∏è Output Requirements:
Provide only SQL queries for each test case,
Each query must return zero rows when data is correct,
Add a short comment above each query explaining what it validates.
