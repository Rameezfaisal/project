AnalysisException                         Traceback (most recent call last)
Cell In[49], line 38
     30 cond_is_open = (
     31     (F.coalesce(F.col("prst.wbstk"), F.lit('')) != 'C') &
     32     (F.coalesce(F.col("prst.abgru"), F.lit('')) == '') &
     33     (F.coalesce(F.col("prst.lfsta"), F.lit('')) != '') &
     34     (expr_open_qty_val > 0)
     35 )
     37 # Apply Filter: WERKS >= 1900 AND Exclude specific Order Types AND (History OR Open)
---> 38 df_filtered = df_joined.filter(
     39     (F.col("prst.werks") >= '1900') & 
     40     (~F.col("prst.auart").isin('ZVC', 'ZR07')) & 
     41     (cond_history | cond_is_open)
     42 )
     44 # --- 3. Define Business Columns (Region, Groups, Prices) ---
     45 
     46 # OrderGroup
     47 col_OrderGroup = (
     48     F.when(F.col("prst.auart").isin('ZAS','ZAV','ZFD','ZO11','ZSA','ZSB','ZSR','RK','ZCR1','ZDR1'), 'OTH')
     49      .when(F.col("prst.auart").isin('AG','ZQ01','ZQ03','ZQ04','ZQFO','ZQIS','ZQUP','ZQUX','ZUQT'), 'QT')
   (...)
     53      .otherwise('SO')
     54 )

File /opt/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:3331, in DataFrame.filter(self, condition)
   3329     jdf = self._jdf.filter(condition)
   3330 elif isinstance(condition, Column):
-> 3331     jdf = self._jdf.filter(condition._jc)
   3332 else:
   3333     raise PySparkTypeError(
   3334         error_class="NOT_COLUMN_OR_STR",
   3335         message_parameters={"arg_name": "condition", "arg_type": type(condition).__name__},
   3336     )

File ~/cluster-env/trident_env/lib/python3.11/site-packages/py4j/java_gateway.py:1322, in JavaMember.__call__(self, *args)
   1316 command = proto.CALL_COMMAND_NAME +\
   1317     self.command_header +\
   1318     args_command +\
   1319     proto.END_COMMAND_PART
   1321 answer = self.gateway_client.send_command(command)
-> 1322 return_value = get_return_value(
   1323     answer, self.gateway_client, self.target_id, self.name)
   1325 for temp_arg in temp_args:
   1326     if hasattr(temp_arg, "_detach"):

File /opt/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py:185, in capture_sql_exception.<locals>.deco(*a, **kw)
    181 converted = convert_exception(e.java_exception)
    182 if not isinstance(converted, UnknownException):
    183     # Hide where the exception came from that shows a non-Pythonic
    184     # JVM exception message.
--> 185     raise converted from None
    186 else:
    187     raise

AnalysisException: [AMBIGUOUS_REFERENCE] Reference `prst`.`erdat` is ambiguous, could be: [`prst`.`erdat`, `prst`.`erdat`].
