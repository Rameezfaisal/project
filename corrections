​Cell 4: Lookup Data Preparation (The "Fan-out" Fix)
​Business Context:
This cell reads the secondary tables (like Plant Data, Pricing, Notifications).
Crucially, it solves the "Row Explosion" issue you faced earlier (14M \to 56M) by pre-aggregating data that has a "Many-to-One" relationship with Sales Orders.
​VEKP (Handling Units): We group by Order and take the latest tracking number.
​QMEL (Notifications): We group by Order and take the latest notification number.
​AFKO (Production): We select the primary activity type per order, ensuring unique keys.
​Pruning: If you set the IP_DATA_PRUNING_FLAG to 'N', these tables are skipped entirely to save time.






# Cell 4: Lookup Data Preparation & Aggregation

flags = params["IP_DATA_PRUNING_FLAG"]
# Flag Positions: 1:MARC, 2:MBEW, 3:VEKP, 4:VBKD, 5:AFVC, 6:QMEL, 7:T024X

# --- 1. MARC (Plant Data) ---
if flags[0] == 'Y':
    marc_raw = spark.read.table(paths["marc"])
    # Optimization: Only fetch MARC records for Materials/Plants actually in our filtered Sales Order list
    # This acts as a "Semi-Join" filter to reduce MARC volume significantly.
    marc_df = marc_raw.join(prst_filtered, 
                            (marc_raw.matnr == prst_filtered.vbap_matnr) & 
                            (marc_raw.werks == prst_filtered.vbap_werks), "semi") \
                      .select("matnr", "werks", "dispo", "ekgrp") \
                      .withColumnRenamed("dispo", "marc_dispo") \
                      .withColumnRenamed("ekgrp", "marc_ekgrp")
else:
    marc_df = None

# --- 2. MBEW (Valuation/Costing) ---
if flags[1] == 'Y':
    mbew_raw = spark.read.table(paths["mbew"])
    
    # Filter for Plant 2000
    mbew_2000 = mbew_raw.filter(F.col("bwkey") == '2000') \
                        .select(F.col("matnr"), F.col("stprs").alias("stprs_2000"))
    
    # Filter for Plant 3120
    mbew_3120 = mbew_raw.filter(F.col("bwkey") == '3120') \
                        .select(F.col("matnr"), F.col("stprs").alias("stprs_3120"))
else:
    mbew_2000 = None
    mbew_3120 = None

# --- 3. VEKP (Handling Units) - [FIX: FAN-OUT PREVENTION] ---
if flags[2] == 'Y':
    vekp_raw = spark.read.table(paths["vekp"])
    # SAP Logic was: Group By VPOBJKEY, Max EXIDV2. 
    # This guarantees 1 row per Order, preventing duplication.
    vekp_df = vekp_raw.groupBy("vpobjkey").agg(F.max("exidv2").alias("vekp_track"))
else:
    vekp_df = None

# --- 5. AFKO/ACT (Production Data) - [FIX: FAN-OUT PREVENTION] ---
if flags[4] == 'Y':
    afko_raw = spark.read.table(paths["afko"])
    afvc_raw = spark.read.table(paths["afvc"])
    
    # Step A: Get Distinct Header (Min Operation Number per Order)
    afvc_header = afvc_raw.groupBy("aufpl").agg(F.min("vornr").alias("vornr"))
    
    # Step B: Join back to get Activity Type (LARNT)
    act_df = afvc_header.join(afvc_raw, ["aufpl", "vornr"], "inner") \
                        .select("aufpl", F.col("larnt").alias("activitytype"))
                        
    # Step C: Join with AFKO
    afko_final = afko_raw.join(act_df, "aufpl", "inner") \
                         .select("aufnr", "activitytype")
else:
    afko_final = None

# --- 6. QMEL (Notifications) - [FIX: FAN-OUT PREVENTION] ---
if flags[5] == 'Y':
    qmel_raw = spark.read.table(paths["qmel"])
    # RISK: A single Order can have multiple notifications. 
    # FIX: Take the latest notification (Max QMNUM) per Order (AUFNR).
    qmel_df = qmel_raw.groupBy("aufnr").agg(F.max("qmnum").alias("srvcnotif"))
else:
    qmel_df = None

# --- 4. VBKD (Incoterms) ---
if flags[3] == 'Y':
    vbkd_raw = spark.read.table(paths["vbkd"])
    
    # Header Incoterm (POSNR = '000000') - Unique by Order
    hd_inco = vbkd_raw.filter(F.col("posnr") == '000000') \
                      .select("vbeln", "inco1", "kursk", "prsdt") \
                      .withColumnRenamed("inco1", "hd_inco1")
    
    # Line Incoterm - Unique by Order + Item
    ln_inco = vbkd_raw.select("vbeln", "posnr", "ihrez", "inco1", "zterm") \
                      .withColumnRenamed("inco1", "ln_inco1") \
                      .withColumnRenamed("ihrez", "ln_ihrez") \
                      .withColumnRenamed("zterm", "ln_zterm")
else:
    hd_inco = None
    ln_inco = None

# --- 7. T024X (Lab Office) & Common Masters ---
# These are small tables, so we read them regardless of flags
lab_df = spark.read.table(paths["t024x"]).filter(F.col("spras") == 'E') \
                   .select("labor", F.col("lbtxt").alias("laboffice"))

kna1_df = spark.read.table(paths["kna1"]).select("kunnr", F.col("name1").alias("soldtoname"))

mara_df = spark.read.table(paths["mara"]).select("matnr", "matkl", "mstae", "labor")

part_df = spark.read.table(paths["part"]).select("part", F.col("consumable").alias("part_consumable")).distinct()

print("Lookup tables prepared and aggregated.")
