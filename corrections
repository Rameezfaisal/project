from pyspark.sql import functions as F
from pyspark.sql.window import Window

base_df = (
    df_iplm
    .select(
        F.col("name").alias("ob_prno"),
        F.col("part_name").alias("ob_prtno"),
        F.col("mstae").alias("part_status"),
        F.col("state").alias("obpr_state"),
        F.col("disposition"),
        F.col("reason"),
        F.col("is_the_last_time_buy_available").alias("is_ltb_avail"),
        F.col("ob_last_buy_date").alias("ltb_date"),
        F.col("lrc_suppliers_date_to_zero_inventory").alias("supplier_date_to_zero")
    )
    .filter(F.col("ob_prtno") != "NA")
    .filter(F.upper("obpr_state") != "CLOSED")
    .filter(
        (F.upper("obpr_state").isin("CONFIRMED","IN REVIEW","IN WORK")) |
        (F.upper("disposition").isin("CONFIRMED","DEFER"))
    )
    .filter(F.upper("reason") == "OBSOLETE COMPONENT")
)

obprdtls_df = (
    base_df
    # join with obprlst (this join is fine, names match)
    .join(obprlst_df, base_df.ob_prno == obprlst_df.ob_prno, "inner")

    # LEFT JOIN with OMAT source â€” EXPLICIT ON CONDITION
    .join(
        df_omat_src,
        (base_df.ob_prno == df_omat_src.obprno) &
        (base_df.ob_prtno == df_omat_src.obprtno),
        "left"
    )
    # keep only new OBPR + PRT combinations
    .filter(df_omat_src.obprno.isNull())

    # window logic
    .withColumn(
        "id",
        F.row_number().over(
            Window.partitionBy("ob_prno").orderBy("ob_prtno")
        )
    )
    .withColumn("ltb_qty", F.lit(0))
    .withColumn("reltd_ob_prs", F.lit(None).cast("string"))
    .withColumn("duplicate_pr_flag", F.lit("No"))
    .withColumn("is_obprt_processed", F.lit("No"))
)

obprdtls_df.createOrReplaceTempView("obprdtls")