# Cell 4 (Updated): Lookup Data Preparation with STRICT Deduplication

flags = params["IP_DATA_PRUNING_FLAG"]

# --- 1. MARC (Plant Data) ---
if flags[0] == 'Y':
    marc_raw = spark.read.table(paths["marc"])
    marc_df = marc_raw.join(prst_filtered, 
                            (marc_raw.matnr == prst_filtered.vbap_matnr) & 
                            (marc_raw.werks == prst_filtered.vbap_werks), "semi") \
                      .select("matnr", "werks", "dispo", "ekgrp") \
                      .withColumnRenamed("dispo", "marc_dispo") \
                      .withColumnRenamed("ekgrp", "marc_ekgrp") \
                      .dropDuplicates(["matnr", "werks"]) # <--- FIX
else:
    marc_df = None

# --- 2. MBEW (Valuation) ---
if flags[1] == 'Y':
    mbew_raw = spark.read.table(paths["mbew"])
    
    # Filter for 2000 & Deduplicate by Material
    mbew_2000 = mbew_raw.filter(F.col("bwkey") == '2000') \
                        .select(F.col("matnr"), F.col("stprs").alias("stprs_2000")) \
                        .dropDuplicates(["matnr"]) # <--- FIX: Handles Split Valuation
    
    # Filter for 3120 & Deduplicate by Material
    mbew_3120 = mbew_raw.filter(F.col("bwkey") == '3120') \
                        .select(F.col("matnr"), F.col("stprs").alias("stprs_3120")) \
                        .dropDuplicates(["matnr"]) # <--- FIX
else:
    mbew_2000 = None
    mbew_3120 = None

# --- 3. VEKP (Handling Units) ---
if flags[2] == 'Y':
    vekp_raw = spark.read.table(paths["vekp"])
    vekp_df = vekp_raw.groupBy("vpobjkey").agg(F.max("exidv2").alias("vekp_track"))
else:
    vekp_df = None

# --- 5. AFKO (Production) ---
if flags[4] == 'Y':
    afko_raw = spark.read.table(paths["afko"])
    afvc_raw = spark.read.table(paths["afvc"])
    
    afvc_header = afvc_raw.groupBy("aufpl").agg(F.min("vornr").alias("vornr"))
    
    act_df = afvc_header.join(afvc_raw, ["aufpl", "vornr"], "inner") \
                        .select("aufpl", F.col("larnt").alias("activitytype")) \
                        .dropDuplicates(["aufpl"]) # <--- FIX
                        
    afko_final = afko_raw.join(act_df, "aufpl", "inner") \
                         .select("aufnr", "activitytype") \
                         .dropDuplicates(["aufnr"]) # <--- FIX
else:
    afko_final = None

# --- 6. QMEL (Notifications) ---
if flags[5] == 'Y':
    qmel_raw = spark.read.table(paths["qmel"])
    qmel_df = qmel_raw.groupBy("aufnr").agg(F.max("qmnum").alias("srvcnotif"))
else:
    qmel_df = None

# --- 4. VBKD (Incoterms) ---
if flags[3] == 'Y':
    vbkd_raw = spark.read.table(paths["vbkd"])
    
    hd_inco = vbkd_raw.filter(F.col("posnr") == '000000') \
                      .select("vbeln", "inco1", "kursk", "prsdt") \
                      .withColumnRenamed("inco1", "hd_inco1") \
                      .dropDuplicates(["vbeln"]) # <--- FIX
    
    ln_inco = vbkd_raw.select("vbeln", "posnr", "ihrez", "inco1", "zterm") \
                      .withColumnRenamed("inco1", "ln_inco1") \
                      .withColumnRenamed("ihrez", "ln_ihrez") \
                      .withColumnRenamed("zterm", "ln_zterm") \
                      .dropDuplicates(["vbeln", "posnr"]) # <--- FIX
else:
    hd_inco = None
    ln_inco = None

# --- 7. T024X & Masters ---
lab_df = spark.read.table(paths["t024x"]).filter(F.col("spras") == 'E') \
                   .select("labor", F.col("lbtxt").alias("laboffice")) \
                   .dropDuplicates(["labor"]) # <--- FIX

kna1_df = spark.read.table(paths["kna1"]).select("kunnr", F.col("name1").alias("soldtoname")) \
                .dropDuplicates(["kunnr"]) # <--- FIX

mara_df = spark.read.table(paths["mara"]).select("matnr", "matkl", "mstae", "labor") \
                .dropDuplicates(["matnr"]) # <--- FIX

part_df = spark.read.table(paths["part"]).select("part", F.col("consumable").alias("part_consumable")) \
                .dropDuplicates(["part"]) # <--- FIX

print("Lookup tables prepared with Strict Deduplication.")
