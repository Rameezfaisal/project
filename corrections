Cell 9: Write to Delta Lake (MERGE / Upsert)
​Business Context:
This final step persists the results. We use a Delta MERGE operation (equivalent to SAP UPSERT).
​Match Logic: If a record exists with the same Sales Order (VBELN), Item (POSNR), and Schedule Line (ETENR), we update it with the new calculations.
​New Records: If it doesn't exist, we insert it.
​Schema Safety: Since we carefully renamed every column in final_df (Cell 8) to match the target table exactly, we can use the UPDATE SET * and INSERT * shorthand, which makes the code cleaner and easier to maintain.






# --- WRITE TO DELTA TABLE ---

# 1. Register the DataFrame as a Temporary View for SQL access
final_df.createOrReplaceTempView("source_updates")

# 2. Execute the MERGE Statement
# Note: We assume the target table 'T_SO_STATUS' already exists with the correct schema.
# If this is the very first run, you might use: final_df.write.format("delta").saveAsTable(T_SO_STATUS)

spark.sql(f"""
MERGE INTO {T_SO_STATUS} AS target
USING source_updates AS source
ON target.VBELN = source.VBELN 
   AND target.POSNR = source.POSNR 
   AND target.ETENR = source.ETENR

WHEN MATCHED THEN
  UPDATE SET *

WHEN NOT MATCHED THEN
  INSERT *
""")

print(f"Merge completed successfully into {T_SO_STATUS}")
