from pyspark.sql import functions as F
from pyspark.sql.window import Window

base_df = (
    df_iplm
    .select(
        F.col("name").alias("ob_prno"),
        F.col("part_name").alias("ob_prtno"),
        F.col("mstae").alias("part_status"),
        F.col("state").alias("obpr_state"),
        F.col("disposition"),
        F.col("reason"),
        F.col("is_the_last_time_buy_available").alias("is_ltb_avail"),
        F.col("ob_last_buy_date").alias("ltb_date"),
        F.col("lrc_suppliers_date_to_zero_inventory").alias("supplier_date_to_zero")
    )
    .filter(F.col("ob_prtno") != "NA")
    .filter(F.upper("obpr_state") != "CLOSED")
    .filter(
        (F.upper("obpr_state").isin("CONFIRMED","IN REVIEW","IN WORK")) |
        (F.upper("disposition").isin("CONFIRMED","DEFER"))
    )
    .filter(F.upper("reason") == "OBSOLETE COMPONENT")
)

b = base_df.alias("b")
o = df_omat_src.alias("o")

obprdtls_df = (
    b
    .join(obprlst_df.alias("l"), F.col("b.ob_prno") == F.col("l.ob_prno"), "inner")
    .join(
        o,
        (F.col("b.ob_prno") == F.col("o.obprno")) &
        (F.col("b.ob_prtno") == F.col("o.obprtno")),
        "left"
    )
    # ✅ disambiguated NULL check
    .filter(F.col("o.obprno").isNull())
    # ✅ remove right-side columns to avoid ambiguity later
    .drop(F.col("o.obprno"))
    .drop(F.col("o.obprtno"))
    .withColumn(
        "id",
        F.row_number().over(
            Window.partitionBy("b.ob_prno").orderBy("b.ob_prtno")
        )
    )
    .withColumn("ltb_qty", F.lit(0))
    .withColumn("reltd_ob_prs", F.lit(None).cast("string"))
    .withColumn("duplicate_pr_flag", F.lit("No"))
    .withColumn("is_obprt_processed", F.lit("No"))
)

obprdtls_df.createOrReplaceTempView("obprdtls")