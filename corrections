---------------------------------------------------------------------------
AnalysisException                         Traceback (most recent call last)
Cell In[49], line 93
     78     return (
     79         F.when(
     80             no_supplier,
   (...)
     83         ).otherwise(F.col(main))
     84     )
     86 # =========================================
     87 # Final UPDATE-style projection
     88 # =========================================
     89 df_stage4 = (
     90     cur
     91     .join(p1, "nha", "left")
     92     .join(p2, "nha", "left")
---> 93     .select(
     94         "nha",
     95         sap_update("cur.suppcd_1900","p1_suppcd_1900","p2_suppcd_1900").alias("suppcd_1900"),
     96         sap_update("cur.suppname_1900","p1_suppname_1900","p2_suppname_1900").alias("suppname_1900"),
     97         sap_update("cur.suppcd_2000","p1_suppcd_2000","p2_suppcd_2000").alias("suppcd_2000"),
     98         sap_update("cur.suppname_2000","p1_suppname_2000","p2_suppname_2000").alias("suppname_2000"),
     99         sap_update("cur.suppcd_3120","p1_suppcd_3120","p2_suppcd_3120").alias("suppcd_3120"),
    100         sap_update("cur.suppname_3120","p1_suppname_3120","p2_suppname_3120").alias("suppname_3120"),
    101         sap_update("cur.suppcd_1050","p1_suppcd_1050","p2_suppcd_1050").alias("suppcd_1050"),
    102         sap_update("cur.suppname_1050","p1_suppname_1050","p2_suppname_1050").alias("suppname_1050"),
    103         sap_update("cur.suppcd_1060","p1_suppcd_1060","p2_suppcd_1060").alias("suppcd_1060"),
    104         sap_update("cur.suppname_1060","p1_suppname_1060","p2_suppname_1060").alias("suppname_1060"),
    105         sap_update("cur.suppcd_1090","p1_suppcd_1090","p2_suppcd_1090").alias("suppcd_1090"),
    106         sap_update("cur.suppname_1090","p1_suppname_1090","p2_suppname_1090").alias("suppname_1090")
    107     )
    108 )
    110 df_stage4.createOrReplaceTempView("stage4_basepart_filled")

File /opt/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:3229, in DataFrame.select(self, *cols)
   3184 def select(self, *cols: "ColumnOrName") -> "DataFrame":  # type: ignore[misc]
   3185     """Projects a set of expressions and returns a new :class:`DataFrame`.
   3186 
   3187     .. versionadded:: 1.3.0
   (...)
   3227     +-----+---+
   3228     """
-> 3229     jdf = self._jdf.select(self._jcols(*cols))
   3230     return DataFrame(jdf, self.sparkSession)

File ~/cluster-env/trident_env/lib/python3.11/site-packages/py4j/java_gateway.py:1322, in JavaMember.__call__(self, *args)
   1316 command = proto.CALL_COMMAND_NAME +\
   1317     self.command_header +\
   1318     args_command +\
   1319     proto.END_COMMAND_PART
   1321 answer = self.gateway_client.send_command(command)
-> 1322 return_value = get_return_value(
   1323     answer, self.gateway_client, self.target_id, self.name)
   1325 for temp_arg in temp_args:
   1326     if hasattr(temp_arg, "_detach"):

File /opt/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py:185, in capture_sql_exception.<locals>.deco(*a, **kw)
    181 converted = convert_exception(e.java_exception)
    182 if not isinstance(converted, UnknownException):
    183     # Hide where the exception came from that shows a non-Pythonic
    184     # JVM exception message.
--> 185     raise converted from None
    186 else:
    187     raise

AnalysisException: [AMBIGUOUS_REFERENCE] Reference `cur`.`suppcd_1900` is ambiguous, could be: [`cur`.`suppcd_1900`, `cur`.`suppcd_1900`].
