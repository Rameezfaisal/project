from pyspark.sql import functions as F
from pyspark.sql.window import Window
from pyspark.sql.types import *

# --- 1. TABLE PATH PARAMETERS (Sources) ---
# Core Sales Documents
S_VBAK  = "your_lakehouse.ecc_vbak"  # Sales Doc Header
S_VBAP  = "your_lakehouse.ecc_vbap"  # Sales Doc Item
S_VBEP  = "your_lakehouse.ecc_vbep"  # Schedule Lines
S_VBUK  = "your_lakehouse.ecc_vbuk"  # Header Status
S_VBUP  = "your_lakehouse.ecc_vbup"  # Item Status

# Delivery & Flow
S_LIKP  = "your_lakehouse.ecc_likp"  # Delivery Header
S_LIPS  = "your_lakehouse.ecc_lips"  # Delivery Item
S_VBFA  = "your_lakehouse.ecc_vbfa"  # Document Flow

# Partners & Address
S_VBPA  = "your_lakehouse.ecc_vbpa"  # Partners
S_KNA1  = "your_lakehouse.ecc_kna1"  # Customer Master

# Status & Material Management
S_JEST  = "your_lakehouse.ecc_jest"  # Object Status
S_TJ30T = "your_lakehouse.ecc_tj30t" # Status Texts
S_MARD  = "your_lakehouse.ecc_mard"  # Material Storage Location

# Custom / Calculated Views
S_CSBG  = "your_lakehouse.cvns_csbg_spares_deliveries_new" 

# Control & Config (From INIT Procedure)
S_CONFIG = "your_lakehouse.zt_config" 

# --- 2. TARGET TABLE PARAMETERS ---
T_SO_STATUS = "your_lakehouse.zt_m_sales_order_status_v2"

# --- 3. BUSINESS CONSTANTS (From SAP Logic) ---
CONST_LANG_KEY   = 'E'         # SPRAS = 'E'
CONST_STATUS_PRF = 'Z0000003'  # STSMA = 'Z0000003'
CONST_WERKS_MIN  = '1900'      # Filter for Plants >= 1900

# --- 4. EXECUTION SCOPE ---
# Define the Range of Sales Orders (VBELN) to process in this run.
# In a pipeline, these might be passed as widgets or variables.
PARAM_LOW_VBELN  = "0000000000"
PARAM_HIGH_VBELN = "9999999999"

# --- 5. UTILITY FUNCTIONS ---
def read_and_lower(table_path):
    """Reads a table and ensures all column names are lowercase immediately."""
    df = spark.read.table(table_path)
    return df.select([F.col(c).alias(c.lower()) for c in df.columns])
