from pyspark.sql import functions as F
from pyspark.sql import Window





# ===============================
# Lakehouse table paths
# ===============================

# ---- Source tables (CVs replicated into Lakehouse) ----
src_omat_dmd_tbl          = "eng_test.cv_omat_dmd_consumption_dtls"
src_inforec_tbl           = "eng_test.cv_zt_inforec_data_sched"
src_eord_tbl              = "eng_test.cv_eord"
src_lfa1_tbl              = "eng_test.cv_lfa1"
src_zpo_hist_tbl          = "eng_test.cv_zpo_hstry"
src_ekpo_tbl              = "eng_test.cv_ekpo"
src_iplm_tbl              = "eng_test.cv_iplm_problem_report_part"

# ---- Target tables ----
tgt_suppliers_tbl         = "eng_test.rpt_omat_buynha_suppliers_plants"
tgt_log_tbl               = "eng_test.rpt_omat_log_dtls"

# ---- Control values ----
procurement_plants = ["1900", "2000", "3120", "1050", "1060", "1090"]
fallback_plants    = ["1000", "1010", "1020"]

valid_mrp          = "1"
valid_autet        = "1"
preferred_flag    = "X"

# supplier categories to exclude
excluded_supcats   = ["A05", "A08", "A15"]

# Program name for logging
program_name = "SP_OMAT_UPDATE_SUPPLIERS_BYPLANT"








