from pyspark.sql.functions import rand, floor

df_sup_skew_safe = (
    df_sup
    .withColumn("salt_cd", floor(rand() * 20))   # 20-way split
    .repartition(200, "salt_cd")                 # Fabric-optimized shuffle
    .drop("salt_cd")
)



df_insert = (
    df_sup_skew_safe
    .select(
        F.col("nha_cd").alias("nha"),
        "suppcd_1900","suppname_1900",
        "suppcd_2000","suppname_2000",
        "suppcd_3120","suppname_3120",
        "suppcd_1050","suppname_1050",
        "suppcd_1060","suppname_1060",
        "suppcd_1090","suppname_1090"
    )
    .filter(F.col("nha").isNotNull())
)

df_insert.write.mode("append").format("delta").saveAsTable(tgt_suppliers)