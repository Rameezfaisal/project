AnalysisException                         Traceback (most recent call last)
Cell In[64], line 12
      1 so = (
      2     prst_run
      3     # ---- Standard cost ----
      4     .join(mbew_2000, prst_run["vbap_matnr"] == mbew_2000["matnr_cd"], "left")
      5     .join(mbew_3120, prst_run["vbap_matnr"] == mbew_3120["matnr_cd"], "left")
      6 
      7     # ---- Tracking ----
      8     .join(vekp_track, prst_run["f1_vbeln"] == vekp_track["vpobjkey"], "left")
      9 
     10     # ---- Incoterms ----
     11     .join(hd_incoterm, prst_run["vbeln"] == hd_incoterm["vbeln_cd"], "left")
---> 12     .join(ln_incoterm,
     13           (prst_run["vbeln"] == ln_incoterm["vbeln_cd"]) &
     14           (prst_run["posnr"] == ln_incoterm["posnr_cd"]),
     15           "left")
     16 
     17     # ---- Lab office ----
     18     .join(lab, prst_run["vbap_matnr"] == lab["labor"], "left")
     19 
     20     # ---- Activity ----
     21     .join(afvc_act, prst_run["vbak_aufnr"] == afvc_act["aufpl"], "left")
     22 
     23     # ---- Service Notification ----
     24     .join(qmel_notif, prst_run["vbeln"] == qmel_notif["vbeln"], "left")
     25 )
     27 so.createOrReplaceTempView("so")

File /opt/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:2493, in DataFrame.join(self, other, on, how)
   2491         on = self._jseq([])
   2492     assert isinstance(how, str), "how should be a string"
-> 2493     jdf = self._jdf.join(other._jdf, on, how)
   2494 return DataFrame(jdf, self.sparkSession)

File ~/cluster-env/trident_env/lib/python3.11/site-packages/py4j/java_gateway.py:1322, in JavaMember.__call__(self, *args)
   1316 command = proto.CALL_COMMAND_NAME +\
   1317     self.command_header +\
   1318     args_command +\
   1319     proto.END_COMMAND_PART
   1321 answer = self.gateway_client.send_command(command)
-> 1322 return_value = get_return_value(
   1323     answer, self.gateway_client, self.target_id, self.name)
   1325 for temp_arg in temp_args:
   1326     if hasattr(temp_arg, "_detach"):

File /opt/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py:185, in capture_sql_exception.<locals>.deco(*a, **kw)
    181 converted = convert_exception(e.java_exception)
    182 if not isinstance(converted, UnknownException):
    183     # Hide where the exception came from that shows a non-Pythonic
    184     # JVM exception message.
--> 185     raise converted from None
    186 else:
    187     raise

AnalysisException: Column vbeln_cd#9748, posnr_cd#9749 are ambiguous. It's probably because you joined several Datasets together, and some of these Datasets are the same. This column points to one of the Datasets but Spark is unable to figure out which one. Please alias the Datasets with different names via `Dataset.as` before joining them, and specify the column using qualified name, e.g. `df.as("a").join(df.as("b"), $"a.id" > $"b.id")`. You can also set spark.sql.analyzer.failAmbiguousSelfJoin to false to disable this check.
