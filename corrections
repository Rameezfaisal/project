# #### Step-12 â€” Write to OMAT Tables + Log Execution

# 1. Smart Deduplication Logic
# Instead of random dropDuplicates, we Rank rows by "Data Completeness"
from pyspark.sql import Window

# We prioritize:
# 1. Rows that have a PR Supplier (Critical for Obsolescence)
# 2. Rows that have a PO Supplier (Active purchasing)
# 3. Rows that have a Plant 2000 Supplier (Business Priority)
w = Window.partitionBy("nha").orderBy(
    F.col("suppcd_pr").desc(),      # Non-null PR suppliers float to top
    F.col("suppcd_po1").desc(),     # Non-null PO suppliers float to top
    F.col("suppcd_2000").desc()     # Non-null Plant 2000 suppliers float to top
)

df_final_clean = (
    spark.table("stage7_final_suppliers")
    .withColumn("rn", F.row_number().over(w))
    .filter(F.col("rn") == 1) # Keep only the single best row
    .drop("rn")
)

# 2. Replace target table (full refresh)
(
    df_final_clean
    .select(
        "nha",
        "omat_selected_plant",
        "omat_selected_supplier",
        "omat_selected_suppliercd",
        "suppcd_1050",
        "suppcd_1060",
        "suppcd_1090",
        "suppcd_1900",
        "suppcd_2000",
        "suppcd_3120",
        "suppcd_po1",
        "suppcd_po2",
        "suppcd_pr",
        "suppname_1050",
        "suppname_1060",
        "suppname_1090",
        "suppname_1900",
        "suppname_2000",
        "suppname_3120",
        "suppname_po1",
        "suppname_po2",
        "suppname_pr"
    )
    .write
    .mode("overwrite")
    .format("delta")
    .option("overwriteSchema", "true")
    .saveAsTable(tgt_suppliers_tbl)
)
