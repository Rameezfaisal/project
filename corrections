Cell 1: Environment Setup & Parameterization
â€‹In this cell, we define the environment configurations. In business terms, we are setting the "addresses" for all our source SAP tables and the target status table. We also define the filter range for the Sales Orders (VBELN), which replaces the manual batching logic from the SAP INIT procedure.





from pyspark.sql import functions as F
from pyspark.sql.window import Window
from pyspark.sql.types import *

# --- 1. TABLE PATH PARAMETERS ---
# Source SAP Tables (Assuming they reside in your Lakehouse)
# Replace 'your_lakehouse' with the actual name or use relative paths
S_VBEP  = "your_lakehouse.ecc_vbp"
S_VBAK  = "your_lakehouse.ecc_vbak"
S_VBAP  = "your_lakehouse.ecc_vbap"
S_VBUK  = "your_lakehouse.ecc_vbuk"
S_VBUP  = "your_lakehouse.ecc_vbup"
S_VBFA  = "your_lakehouse.ecc_vbfa"
S_LIKP  = "your_lakehouse.ecc_likp"
S_LIPS  = "your_lakehouse.ecc_lips"
S_VBPA  = "your_lakehouse.ecc_vbpa"
S_KNA1  = "your_lakehouse.ecc_kna1"
S_JEST  = "your_lakehouse.ecc_jest"
S_TJ30T = "your_lakehouse.ecc_tj30t"
S_MARD  = "your_lakehouse.ecc_mard"
S_CSBG  = "your_lakehouse.cvns_csbg_spares_deliveries_new"

# Target Delta Table
T_SO_STATUS = "your_lakehouse.zt_m_sales_order_status_v2"

# --- 2. EXECUTION PARAMETERS ---
# Range filter to replace the SAP BATCH_LIMIT/ITERATION_LIMIT logic
# For a full run, these can be set to wide ranges or handled via a control table
PARAM_LOW_VBELN  = "0000000000"
PARAM_HIGH_VBELN = "9999999999"

# --- 3. UTILITY FUNCTIONS ---
def read_and_lower(table_path):
    """Reads a table and ensures all column names are lowercase immediately."""
    df = spark.read.table(table_path)
    return df.select([F.col(c).alias(c.lower()) for c in df.columns])
