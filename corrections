# #### Step-3b -- Re-establish Scope (NHALIST)
# CRITICAL FIX: We must only process the parts identified in this specific run.
# Without this, the notebook re-processes the entire historical table every time.

nha_source = (
    spark.table("lhs_glb.omat_test.rpt_omat_obpn_buy_nha_dtls")
         .select("cmpprtno", "nha", "isprocessed")
         .filter(F.col("isprocessed") == "N")
         .distinct()
)

# This list matches the exact scope used in Notebook 1 (po_1)
nhalist = nha_source.select("cmpprtno", "nha").distinct()





# #### Step-5 -- Base join graph (Scoped to NHALIST)
# Updated to perform an INNER JOIN on nhalist first.

base = (
    dmd.alias("a")
    # CRITICAL FIX: Filter dmd table to only include the current batch
    .join(nhalist, (dmd.cmpprtno == nhalist.cmpprtno) & (dmd.nha == nhalist.nha), "inner")
    
    # Continue with original joins...
    .join(sup.alias("b"), dmd.nha == sup.nha, "left")
    .join(makt, dmd.nha == makt.matnr, "left")
    
    # Note: Added trim/casting safeguards often needed for SAP keys
    .join(marc, (dmd.nha == marc.matnr) & (sup.omat_selected_plant == marc.werks), "left")
    .join(mbew, (dmd.nha == mbew.matnr) & (sup.omat_selected_plant == mbew.bwkey), "left")
    
    .join(critical, dmd.nha == critical.nha_cd, "left")
    .join(ce, dmd.nha == ce.nha_cd, "left")
    
    .join(xref, sup.omat_selected_suppliercd == xref.lifnr, "left")
    
    .join(code.alias("g"), xref.zcommcod == F.col("g.zcode"), "left")
    .join(code.alias("h"), xref.zgrpsmg == F.col("h.zcode"), "left")
    .join(code.alias("i"), xref.zsmgcd == F.col("i.zcode"), "left")
    .join(code.alias("j"), xref.zprimmcd == F.col("j.zcode"), "left")
)








