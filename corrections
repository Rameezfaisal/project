# #### Step-7 â€” Base-Part Supplier Inheritance for R / S / C / X / D Parts
# -------------------------------------------------------------------------
# CORRECTION APPLIED:
# 1. Deduplicated the 'lookup' table to prevent Cartesian joins (1-to-Many).
# 2. Filtered 'base' patterns to only process rows that actually need updates.
# 3. Excluded empty strings from join keys to prevent massive grouping on empty keys.
# -------------------------------------------------------------------------

# ===============================
# Base table
# ===============================
base = spark.table("stage3_deduped")

# ===============================
# Condition: Missing Suppliers
# ===============================
no_supplier_cond = (
    (F.length(F.trim(F.col("suppcd_1900"))) == 0) &
    (F.length(F.trim(F.col("suppcd_2000"))) == 0) &
    (F.length(F.trim(F.col("suppcd_3120"))) == 0) &
    (F.length(F.trim(F.col("suppcd_1050"))) == 0) &
    (F.length(F.trim(F.col("suppcd_1060"))) == 0) &
    (F.length(F.trim(F.col("suppcd_1090"))) == 0)
)

# ===============================
# 1. Create a SAFE Lookup Table (Unique Keys Only)
# ===============================
# We drop duplicates on 'nha' to ensure we never trigger a 1-to-Many join explosion.
lookup = (
    base
    .select(
        F.col("nha").alias("base_nha"),
        "suppcd_1900","suppname_1900",
        "suppcd_2000","suppname_2000",
        "suppcd_3120","suppname_3120",
        "suppcd_1050","suppname_1050",
        "suppcd_1060","suppname_1060",
        "suppcd_1090","suppname_1090"
    )
    .dropDuplicates(["base_nha"]) # <--- CRITICAL FIX
)

# ===============================
# 2. Prepare Pattern Matches (P1 & P2)
# ===============================
# We only calculate this for rows that meet the 'no_supplier_cond'
# This drastically reduces the volume of data entering the join.

df_needs_update = base.filter(no_supplier_cond).select("nha")

# --- Pattern 1: Replace [RSCXD] with '-' ---
p1_map = (
    df_needs_update
    .withColumn("base_nha", F.regexp_replace("nha", "[RSCXD]", "-"))
    .filter(F.length(F.trim("base_nha")) > 0)      # Safety: No empty keys
    .filter(F.col("base_nha") != F.col("nha"))     # Optimization: Skip if no change
    .join(lookup, "base_nha", "inner")             # Inner join reduces dataset
    .drop("base_nha")
    .selectExpr(
        "nha",
        "suppcd_1900 as p1_suppcd_1900", "suppname_1900 as p1_suppname_1900",
        "suppcd_2000 as p1_suppcd_2000", "suppname_2000 as p1_suppname_2000",
        "suppcd_3120 as p1_suppcd_3120", "suppname_3120 as p1_suppname_3120",
        "suppcd_1050 as p1_suppcd_1050", "suppname_1050 as p1_suppname_1050",
        "suppcd_1060 as p1_suppcd_1060", "suppname_1060 as p1_suppname_1060",
        "suppcd_1090 as p1_suppcd_1090", "suppname_1090 as p1_suppname_1090"
    )
)

# --- Pattern 2: Remove [RSCXD] completely ---
p2_map = (
    df_needs_update
    .withColumn("base_nha", F.regexp_replace("nha", "[RSCXD]", ""))
    .filter(F.length(F.trim("base_nha")) > 0)      # Safety: No empty keys
    .filter(F.col("base_nha") != F.col("nha"))     # Optimization: Skip if no change
    .join(lookup, "base_nha", "inner")
    .drop("base_nha")
    .selectExpr(
        "nha",
        "suppcd_1900 as p2_suppcd_1900", "suppname_1900 as p2_suppname_1900",
        "suppcd_2000 as p2_suppcd_2000", "suppname_2000 as p2_suppname_2000",
        "suppcd_3120 as p2_suppcd_3120", "suppname_3120 as p2_suppname_3120",
        "suppcd_1050 as p2_suppcd_1050", "suppname_1050 as p2_suppname_1050",
        "suppcd_1060 as p2_suppcd_1060", "suppname_1060 as p2_suppname_1060",
        "suppcd_1090 as p2_suppcd_1090", "suppname_1090 as p2_suppname_1090"
    )
)

# ===============================
# 3. Apply Updates to Base
# ===============================
# Helper to coalesce: Existing -> Pattern 1 -> Pattern 2
def coalesce_logic(curr_col, p1_col, p2_col):
    return (
        F.when(
            (F.length(F.trim(curr_col)) > 0), 
            F.col(curr_col) # Keep existing if present
        )
        .otherwise(
            F.coalesce(F.col(p1_col), F.col(p2_col)) # Fallback to P1, then P2
        )
    )

df_stage4 = (
    base
    .join(p1_map, "nha", "left")
    .join(p2_map, "nha", "left")
    .select(
        F.col("nha"),
        coalesce_logic("suppcd_1900", "p1_suppcd_1900", "p2_suppcd_1900").alias("suppcd_1900"),
        coalesce_logic("suppname_1900", "p1_suppname_1900", "p2_suppname_1900").alias("suppname_1900"),
        
        coalesce_logic("suppcd_2000", "p1_suppcd_2000", "p2_suppcd_2000").alias("suppcd_2000"),
        coalesce_logic("suppname_2000", "p1_suppname_2000", "p2_suppname_2000").alias("suppname_2000"),
        
        coalesce_logic("suppcd_3120", "p1_suppcd_3120", "p2_suppcd_3120").alias("suppcd_3120"),
        coalesce_logic("suppname_3120", "p1_suppname_3120", "p2_suppname_3120").alias("suppname_3120"),
        
        coalesce_logic("suppcd_1050", "p1_suppcd_1050", "p2_suppcd_1050").alias("suppcd_1050"),
        coalesce_logic("suppname_1050", "p1_suppname_1050", "p2_suppname_1050").alias("suppname_1050"),
        
        coalesce_logic("suppcd_1060", "p1_suppcd_1060", "p2_suppcd_1060").alias("suppcd_1060"),
        coalesce_logic("suppname_1060", "p1_suppname_1060", "p2_suppname_1060").alias("suppname_1060"),
        
        coalesce_logic("suppcd_1090", "p1_suppcd_1090", "p2_suppcd_1090").alias("suppcd_1090"),
        coalesce_logic("suppname_1090", "p1_suppname_1090", "p2_suppname_1090").alias("suppname_1090")
    )
)

df_stage4.createOrReplaceTempView("stage4_basepart_filled")
