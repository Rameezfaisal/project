# #### Step-12 â€” Write to OMAT Tables + Log Execution

# 1. Final dataset
# Add dropDuplicates(["nha"]) to ensure strict 1-to-1 parity with SAP
df_final = spark.table("stage7_final_suppliers").dropDuplicates(["nha"]) 

# 2. Replace target table (full refresh)
(
    df_final
    .select(
        "nha",
        "omat_selected_plant",
        "omat_selected_supplier",
        "omat_selected_suppliercd",
        "suppcd_1050",
        "suppcd_1060",
        "suppcd_1090",
        "suppcd_1900",
        "suppcd_2000",
        "suppcd_3120",
        "suppcd_po1",
        "suppcd_po2",
        "suppcd_pr",
        "suppname_1050",
        "suppname_1060",
        "suppname_1090",
        "suppname_1900",
        "suppname_2000",
        "suppname_3120",
        "suppname_po1",
        "suppname_po2",
        "suppname_pr"
    )
    .write
    .mode("overwrite")
    .format("delta")
    .option("overwriteSchema", "true")
    .saveAsTable(tgt_suppliers_tbl)
)








from pyspark.sql import Window

# Define a window partitioned by NHA, ordered by "Data Quality"
# We prioritize rows that have a PR supplier, then PO supplier, then MRP supplier
w = Window.partitionBy("nha").orderBy(
    F.col("suppcd_pr").desc(),      # Prioritize if it has a PR supplier
    F.col("suppcd_po1").desc(),     # Then if it has a PO supplier
    F.col("suppcd_2000").desc()     # Then if it has a Plant 2000 supplier
)

# Keep only the "Rank 1" row
df_final = (
    spark.table("stage7_final_suppliers")
    .withColumn("rn", F.row_number().over(w))
    .filter(F.col("rn") == 1)
    .drop("rn")
)
