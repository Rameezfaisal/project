spark.sql(f"DELETE FROM {tgt_dtls}")


final_dtls = (
    lstdata.alias("a")
    .join(makt_df.alias("m"), F.col("m.matnr") == F.col("a.nha"), "left")
    .join(sup_df.alias("s"), F.col("s.nha") == F.col("a.nha"), "left")
    .join(eco_df.alias("e"), (F.col("e.nha")==F.col("a.dash")) & (F.col("e.obprtno")==F.col("a.cmpprtno")), "left")
    .join(mara_df.alias("ma"), F.col("ma.matnr")==F.col("a.nha"), "left")
    .filter(~F.col("ma.mstae").isin("OB","OS","OP"))
    .select(
        "a.cmpprtno","a.nha","a.type","a.dash",
        F.col("m.maktx").alias("description"),
        "e.cmpqpa","ma.matkl","ma.mstae",
        F.col("s.omat_selected_suppliercd").alias("vencode"),
        F.col("s.omat_selected_supplier").alias("venname"),
        F.col("s.plifz").alias("lead_time")
    )
)


final_dtls.write.insertInto(tgt_dtls)










partcons = (
    spark.table(tgt_dtls).alias("a")
    .join(dmd_df.alias("b"), F.col("a.nha")==F.col("b.hostpartid"))
    .filter(F.col("b.ordertype").isin("TA","ZCON","ZO09","ZO04","ZSD","ZSO","ZFO","ZUP"))
    .groupBy("a.nha")
    .agg(F.sum("b.historyamount").alias("sprs_cons"))
)

