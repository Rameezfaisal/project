

spark.conf.set("spark.sql.legacy.timeParserPolicy", "CORRECTED")





from pyspark.sql import functions as F
from pyspark.sql.types import *

# --- 1. TABLE PATH PARAMETERS ---
# The Base Fact Table (Silver Layer) we just created
S_FACT_STATUS = "your_lakehouse.zt_m_sales_order_status_v2"

# Master Data / Dimensions (Bronze/Silver Layers)
S_KNA1  = "your_lakehouse.ecc_kna1"    # Customer
S_MARA  = "your_lakehouse.ecc_mara"    # Material Master
S_MARC  = "your_lakehouse.ecc_marc"    # Plant Data
S_MBEW  = "your_lakehouse.ecc_mbew"    # Material Valuation (Cost)
S_T024X = "your_lakehouse.ecc_t024x"   # Lab/Office
S_QMEL  = "your_lakehouse.ecc_qmel"    # Service Notification
S_AFKO  = "your_lakehouse.ecc_afko"    # Order Header (Service)
S_AFVC  = "your_lakehouse.ecc_afvc"    # Operation (Service)
S_VEKP  = "your_lakehouse.ecc_vekp"    # Handling Units (Tracking)
S_VBKD  = "your_lakehouse.ecc_vbkd"    # Sales Business Data (Incoterms)
S_PART  = "your_lakehouse.cv_part"     # Custom Part Attributes

# Target Gold Table
T_GOLD_ANALYSIS = "your_lakehouse.gold_sales_analysis"

# --- 2. BUSINESS LOGIC CONSTANTS ---
RETENTION_DAYS = 732  # Keep 2 Years of History + All Open Orders
CONST_LANG_KEY = 'E'

# --- 3. UTILITY ---
# (Columns are already lowercase in your env, so we just read)

# --- CONFIGURATION (Crucial for SAP Dates) ---
spark.conf.set("spark.sql.parquet.datetimeRebaseModeInWrite", "LEGACY")
spark.conf.set("spark.sql.parquet.int96RebaseModeInWrite", "LEGACY")
spark.conf.set("spark.sql.avro.datetimeRebaseModeInWrite", "LEGACY")









Cell 2: Load Base Fact & Apply Scoping
​Business Context:
Here we load the "Silver" table we just populated. We immediately filter it to reduce the data volume for the subsequent joins.
​Filter 1 (Exclusions): Remove Order Types ZVC and ZR07 (as per SAP logic).
​Filter 2 (Time Scope): We keep a record if:
​It is an 'Open' order (regardless of age).
​OR the Line Item Creation Date (VBAP_ERDAT) is within the last 2 years (732 days).






# --- LOAD BASE FACT ---
df_status = spark.read.table(S_FACT_STATUS)

# --- APPLY SCOPE FILTERS ---
# SAP Logic: WHERE PRST.VBAK_AUART NOT IN ('ZVC','ZR07')
#            AND (PRST.VBAP_ERDAT >= ADD_DAYS(CURRENT_DATE,-732) OR PRST.ORDER_STATUS = 'Open')

df_base = df_status.filter(
    (~F.col("VBAK_AUART").isin("ZVC", "ZR07")) & 
    (
        (F.col("ORDER_STATUS") == "Open") | 
        (F.col("VBAP_ERDAT") >= F.date_sub(F.current_date(), RETENTION_DAYS))
    )
)

# Optimization: Since this is the foundation for all subsequent joins,
# we can hint to Spark that this is our main driving table.
# count = df_base.count() 
# print(f"Base records to enrich: {count}")
