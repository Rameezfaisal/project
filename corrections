from pyspark.sql import functions as F
from pyspark.sql import types as T








# =========================
# Source Tables
# =========================
tbl_buynha_suppliers      = "lakehouse.w_omat_zt_omat_buynha_suppliers_plants"
tbl_eord                  = "lakehouse.cv_eord"
tbl_lfa1                  = "lakehouse.cv_lfa1"
tbl_eban                  = "lakehouse.cv_eban"
tbl_zpo_hist              = "lakehouse.cv_zpo_hstry"
tbl_zpo_hist_arch         = "lakehouse.cv_zpo_hstry_arch"

tbl_dmd                   = "lakehouse.cv_omat_dmd_consumption_dtls"
tbl_part_dtls             = "lakehouse.cv_omat_part_dtls"
tbl_obprtno_dtls          = "lakehouse.cv_omat_obprtno_dtls"
tbl_obpr_info             = "lakehouse.cv_omat_obpr_info_dtls"

tbl_rscxd_dmd             = "lakehouse.cv_omat_rscxd_dmd_cons"
tbl_rscxd_dtls            = "lakehouse.cv_omat_rscxd_dtls"

# =========================
# Target Tables
# =========================
tbl_target_supplier_inv   = "lakehouse.omat_supplier_inventory"
tbl_audit_log             = "lakehouse.omat_refresh_audit"

# =========================
# Control Parameters
# =========================
refresh_frequency_days = 42
valid_states = ["CONFIRMED","IN REVIEW","IN WORK"]









buynha_suppliers_df = spark.table(tbl_buynha_suppliers) \
    .select(
        F.col("nha").alias("nha"),
        F.col("omat_selected_suppliercd").alias("supcode")
    ).distinct()

eord_df = spark.table(tbl_eord) \
    .select(
        F.col("matnr").alias("material"),
        F.lpad(F.col("lifnr"),10,"0").alias("supcode_eord")
    )

lfa1_df = spark.table(tbl_lfa1) \
    .select(
        F.col("lifnr").alias("lifnr_lfa1"),
        F.col("name1").alias("suppliername_lfa1")
    )

dmd_df = spark.table(tbl_dmd).selectExpr(
    "cmpprtno as cmpprtno_dmd",
    "nha as nha_dmd",
    "lamqoh as lamqoh_dmd",
    "open_po_qty as open_po_qty_dmd",
    "restricted_all_stock as restricted_all_stock_dmd"
)

part_dtls_df = spark.table(tbl_part_dtls).selectExpr(
    "cmpprtno as cmpprtno_part",
    "nha as nha_part",
    "vencode as vencode_part",
    "venname as venname_part"
)

obprt_df = spark.table(tbl_obprtno_dtls).selectExpr(
    "obprno","obprtno",
    "upper(obpr_state) as obpr_state",
    "is_ltb_avail","ltb_date","ltb_qty",
    "supplier_date_to_zero as supplier_date_to_zero"
)

rscxd_dmd_df = spark.table(tbl_rscxd_dmd).selectExpr(
    "cmpprtno as cmpprtno_r",
    "nha as nha_r",
    "lamqoh as lamqoh_r",
    "open_po_qty as open_po_qty_r",
    "restricted_all_stock as restricted_all_stock_r"
)

rscxd_dtls_df = spark.table(tbl_rscxd_dtls).selectExpr(
    "cmpprtno as cmpprtno_rd",
    "nha as nha_rd",
    "vencode as vencode_rd",
    "venname as venname_rd"
)








base_data_df = buynha_suppliers_df.groupBy("nha","supcode").count().drop("count")










