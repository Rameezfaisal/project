AnalysisException                         Traceback (most recent call last)
Cell In[67], line 49
     47 df_clean = base
     48 for plant in procurement_plants:
---> 49     df_clean = remove_non_fixed(df_clean, plant)
     51 df_clean.createOrReplaceTempView("stage3_deduped")

Cell In[67], line 37, in remove_non_fixed(df, plant)
     28 def remove_non_fixed(df, plant):
     29     return (
     30         df.join(
     31             df_eord_flag,
     32             (df.nha == df_eord_flag.nha) &
     33             (df_eord_flag.werks_cd == plant) &
     34             (df_eord_flag.lifnr_cd == df[f"suppcd_{plant}"]),
     35             "left"
     36         )
---> 37         .filter(
     38             ~(
     39                 (F.col("nha").isin([r.nha for r in df_dups.collect()])) &
     40                 (F.col("flifn_cd") != preferred_flag) &
     41                 (F.col("autet_cd").isNotNull())
     42             )
     43         )
     44         .drop("werks_cd","lifnr_cd","flifn_cd","autet_cd")
     45     )

File /opt/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:3331, in DataFrame.filter(self, condition)
   3329     jdf = self._jdf.filter(condition)
   3330 elif isinstance(condition, Column):
-> 3331     jdf = self._jdf.filter(condition._jc)
   3332 else:
   3333     raise PySparkTypeError(
   3334         error_class="NOT_COLUMN_OR_STR",
   3335         message_parameters={"arg_name": "condition", "arg_type": type(condition).__name__},
   3336     )

File ~/cluster-env/trident_env/lib/python3.11/site-packages/py4j/java_gateway.py:1322, in JavaMember.__call__(self, *args)
   1316 command = proto.CALL_COMMAND_NAME +\
   1317     self.command_header +\
   1318     args_command +\
   1319     proto.END_COMMAND_PART
   1321 answer = self.gateway_client.send_command(command)
-> 1322 return_value = get_return_value(
   1323     answer, self.gateway_client, self.target_id, self.name)
   1325 for temp_arg in temp_args:
   1326     if hasattr(temp_arg, "_detach"):

File /opt/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py:185, in capture_sql_exception.<locals>.deco(*a, **kw)
    181 converted = convert_exception(e.java_exception)
    182 if not isinstance(converted, UnknownException):
    183     # Hide where the exception came from that shows a non-Pythonic
    184     # JVM exception message.
--> 185     raise converted from None
    186 else:
    187     raise

AnalysisException: [AMBIGUOUS_REFERENCE] Reference `nha` is ambiguous, could be: [`nha`, `stage2_eord_filled`.`nha`].
