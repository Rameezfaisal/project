# #### Step-7 -- Insert ONLY new NHA's (SAP-faithful)
# - Anti-join with target table
# - Matches SAP: LEFT JOIN target WHERE target.NHA IS NULL

# Read existing target NHAs
df_existing_nha = (
    spark.read.table(tgt_suppliers)
    .select("nha")
    .distinct()
)

# Anti-join: NHAs not yet present in target
df_sup_new = (
    df_sup.alias("s")
    .join(
        df_existing_nha.alias("t"),
        F.col("s.nha_cd") == F.col("t.nha"),
        "left_anti"
    )
)

# Prepare insert dataset
df_insert = (
    df_sup_new
    .select(
        F.col("nha_cd").alias("nha"),

        "suppcd_1900","suppname_1900",
        "suppcd_2000","suppname_2000",
        "suppcd_3120","suppname_3120",
        "suppcd_1050","suppname_1050",
        "suppcd_1060","suppname_1060",
        "suppcd_1090","suppname_1090"
    )
    .filter(F.col("nha").isNotNull())
)

# Optional but good for Fabric scale
df_insert_balanced = df_insert.repartitionByRange(300, "nha")

# Insert
(
    df_insert_balanced
    .write
    .mode("append")
    .format("delta")
    .saveAsTable(tgt_suppliers)
)